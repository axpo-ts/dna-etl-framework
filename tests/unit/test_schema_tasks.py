import logging

from pyspark.sql import SparkSession
from pyspark.sql.functions import current_date, current_timestamp, lit

from data_platform.tasks.core import Configuration, TaskContext
from data_platform.tasks.schema.schema import CreateSchemaTask
from data_platform.tasks.schema.table.table import (
    CreateTableFromDataFrameTask,
    CreateTableTask,
)


def test_create_schema_task(spark: SparkSession, logger: logging.Logger, create_schema: str) -> None:
    logger.info("Testing the Create Schema task")

    test_etl_config = Configuration(
        {
            "1": {
                "task_name": "CreateSchemaTask",
                "schema_name": create_schema.lower(),
                "catalog_name": "dna_dev_staging",
            }
        }
    )
    logger.info(test_etl_config)
    task_context = TaskContext(logger=logger, spark=spark, configuration=test_etl_config)
    etl_job = CreateSchemaTask()
    etl_job.execute(context=task_context, conf=test_etl_config.get_tree("1"))

    df = spark.sql("SHOW SCHEMAS").filter(f"databaseName = '{create_schema.lower()}'")
    assert df.count() == 1


def test_create_table_task(spark: SparkSession, logger: logging.Logger, create_schema: str) -> None:
    logger.info("Testing the Simple batch writer task")
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {create_schema}")
    logger.info("Create metadata table")
    test_create_table_config = Configuration(
        {
            "1": {
                "task_name": "CreateTableTask",
                "schema_name": create_schema,
                "table_name": "test_silver_table",
                "table_schema": "Id int, Name String",
            }
        }
    )
    logger.info(test_create_table_config)
    task_context_metadata = TaskContext(logger=logger, spark=spark, configuration=test_create_table_config)
    table_creation = CreateTableTask()

    try:
        table_creation.execute(context=task_context_metadata, conf=test_create_table_config.get_tree("1"))
    except Exception as e:
        error_message = str(e)
        if (
            "table_id BIGINT not null GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),Id int, Name String"  # noqa: E501
            in error_message
        ):
            logger.info("Execution Failed as Auto Identity Doesn't work locally but expected statement validated")
            return
        else:
            raise


def test_create_table_from_dataframe_task(spark: SparkSession, logger: logging.Logger, create_schema: str) -> None:
    logger.info("Testing the Create Table From DataFrame task")

    test_etl_config = Configuration(
        {
            "1": {
                "task_name": "CreateTableFromDataFrameTask",
                "schema_name": create_schema,
                "table_name": "test_table",
                "df_namespace": "test",
                "df_key": "test_key",
            }
        }
    )
    logger.info(test_etl_config)
    task_context = TaskContext(logger=logger, spark=spark, configuration=test_etl_config)

    # create the dataframe
    df = spark.range(1)
    task_context.put_property("test", "test_key", df)
    etl_job = CreateTableFromDataFrameTask()
    etl_job.execute(context=task_context, conf=test_etl_config.get_tree("1"))

    df_out = spark.table(f"{create_schema}.test_table")

    df_out_schema = [(field.name, field.dataType.simpleString()) for field in df_out.schema.fields]
    df_in_schema = [(field.name, field.dataType.simpleString()) for field in df.schema.fields]

    assert df_out.count() == 0
    assert df_out_schema == df_in_schema


def test_create_table_with_partition_from_dataframe_task(
    spark: SparkSession, logger: logging.Logger, create_schema: str
) -> None:
    logger.info("Testing the Create Table From DataFrame along with Partitions task")

    test_etl_config = Configuration(
        {
            "1": {
                "task_name": "CreateTableFromDataFrameTask",
                "schema_name": create_schema,
                "table_name": "test_table_partitions",
                "df_namespace": "test",
                "df_key": "test_key",
                "writer_options": {"partition_cols": "__partition_by", "partition_expression": "to_date(CreatedTime)"},
            }
        }
    )
    logger.info(test_etl_config)
    task_context = TaskContext(logger=logger, spark=spark, configuration=test_etl_config)

    # create the dataframe
    df = spark.range(5).withColumn("CreatedTime", lit(current_timestamp()))
    task_context.put_property("test", "test_key", df)
    etl_job = CreateTableFromDataFrameTask()
    etl_job.execute(context=task_context, conf=test_etl_config.get_tree("1"))

    columns = spark.catalog.listColumns(f"{create_schema}.test_table_partitions")
    partition_columns = [col.name for col in columns if col.isPartition]

    assert "__partition_by" in partition_columns
    assert spark.table(f"{create_schema}.test_table_partitions").count() == 0


def test_create_table_with_partition_primkeys_from_dataframe_task(
    spark: SparkSession, logger: logging.Logger, create_schema: str
) -> None:
    logger.info("Testing the Create Table From DataFrame along with Partitions task")

    test_etl_config = Configuration(
        {
            "1": {
                "task_name": "CreateTableFromDataFrameTask",
                "schema_name": create_schema,
                "table_name": "test_table_partitions_keys",
                "df_namespace": "test",
                "df_key": "test_key",
                "writer_options": {
                    "partition_cols": "__partition_by",
                    "partition_expression": "to_date(CreatedTime)",
                    "primary_keys": "id,CreateDate",
                },
            }
        }
    )
    logger.info(test_etl_config)
    task_context = TaskContext(logger=logger, spark=spark, configuration=test_etl_config)

    df = (
        spark.range(5).withColumn("CreatedTime", lit(current_timestamp())).withColumn("CreateDate", lit(current_date()))
    )
    task_context.put_property("test", "test_key", df)
    etl_job = CreateTableFromDataFrameTask()

    try:
        etl_job.execute(context=task_context, conf=test_etl_config.get_tree("1"))
    except Exception as e:
        error_message = str(e)
        if (
            f"ALTER TABLE `{create_schema}`.`test_table_partitions_keys` ADD CONSTRAINT pk_test_table_partitions_keys PRIMARY KEY (id,CreateDate)"  # noqa: E501
            in error_message
            or "v1 tables cannot specify NOT NULL" in error_message
        ):
            logger.info("Executio Failed as Alter Doesn't work locally but the expected statement has been validated")
            return
        else:
            raise
