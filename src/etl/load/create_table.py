from __future__ import annotations

from collections.abc import Sequence
from dataclasses import dataclass, field

from pyspark.sql.utils import AnalysisException

from data_model import AbstractTableModel
from etl.core import TaskContext
from etl.etl_task import ETLTask


# ────────────────────────────────────────────────────────────────────────────
#                         Single public task
# ────────────────────────────────────────────────────────────────────────────
@dataclass(kw_only=True)
class CreateTable(ETLTask):
    """Create (if necessary) a Delta table described by an `AbstractTableModel`.

    Parameters
    ----------
    context : TaskContext
        Spark / logger wrapper supplied by your pipeline framework.
    table_model   : AbstractTableModel
        The canonical table definition (identifier, schema, metadata …).
    partition_expression : str | None
        Same semantics as before - used only when both a DataFrame
        **and** partition columns are present.
    """

    context: TaskContext
    table_model: AbstractTableModel
    partition_expression: str | None = None
    task_name: str = field(init=False, default="CreateTable")

    @property
    def _full_table_name(self) -> str:
        return self._full_unitycatalog_name_of(self.table_model.identifier)

    def _audit_columns_suffix(self) -> str:
        """Return DDL for audit columns as requested by the model."""
        suffix = []
        for col in self.table_model.audit_columns:
            if col == "created_at":
                suffix.append(
                    "created_at TIMESTAMP NOT NULL DEFAULT current_timestamp() COMMENT "
                    "'DnA Platform internal timestamp of the record creation'"
                )
            elif col == "created_by":
                suffix.append(
                    "created_by STRING NOT NULL DEFAULT current_user() COMMENT "
                    "'DnA Platform internal user that created the record'"
                )
            elif col == "updated_at":
                suffix.append(
                    "updated_at TIMESTAMP NOT NULL DEFAULT current_timestamp() "
                    "COMMENT 'DnA Platform internal timestamp of the last update'"
                )
            elif col == "updated_by":
                suffix.append(
                    "updated_by STRING NOT NULL DEFAULT current_user() "
                    "COMMENT 'DnA Platform internal user that last updated the record'"
                )
            elif col == "table_id":
                suffix.append(
                    "table_id BIGINT NOT NULL GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1) "
                    "COMMENT 'DnA Platform internal unique identifier for the record'"
                )
        return (", " + ", ".join(suffix)) if suffix else ""

    def _constraint_clauses(self) -> list[str]:
        """Build the ALTER-TABLE statements for every CHECK constraint that lives in `table_model.constraints`."""
        full_name = self._full_table_name
        return [
            f"ALTER TABLE {full_name} ADD CONSTRAINT c_{self.table_model.identifier.name}_{name} {expr}; "
            for name, expr in self.table_model.constraints.items()
        ]

    def _add_constraints(self) -> None:
        """Run every constraint statement with basic logging."""
        spark = self.context.spark
        for stmt in self._constraint_clauses():
            self.context.logger.info(stmt)
            spark.sql(stmt)

    def _liquid_cluster_clause(self) -> str:
        """Return the CLUSTER BY clause for liquid clustering, if any."""
        lccs: Sequence[Sequence[str]] = self.table_model.liquid_cluster_cols
        if not self.table_model.partition_cols:
            if not lccs:
                return "CLUSTER BY AUTO"
            else:
                lc_clauses = ", ".join(lccs)
                final_statement = f"CLUSTER BY ({lc_clauses})"
                # lc_clauses = [f"CLUSTER BY ({', '.join(cols)})" for cols in lccs]
                return final_statement
        return ""

    def _partition_clause(self) -> str:
        pcs: Sequence[str] = self.table_model.partition_cols
        return f"PARTITIONED BY ({', '.join(pcs)})" if pcs else ""

    def _comment_clause(self) -> str:
        return (
            f"""
        COMMENT "{self.table_model.comment}"
        """
            if self.table_model.comment
            else ""
        )

    def _add_primary_keys(self) -> None:
        """ALTER TABLE to set NOT NULL + PK (Spark ≤ 3.4)."""
        spark = self.context.spark
        for col in self.table_model.primary_keys:
            sql_stmt = f"ALTER TABLE {self._full_table_name} ALTER COLUMN {col} SET NOT NULL"
            self.context.logger.info(sql_stmt)
            spark.sql(sql_stmt)

        # Add primary key constraint
        sql_stmt = (
            f"ALTER TABLE {self._full_table_name} "
            f"ADD CONSTRAINT pk_{self.table_model.identifier.name} "
            f"PRIMARY KEY ({', '.join(self.table_model.primary_keys)})"
        )
        self.context.logger.info(sql_stmt)
        spark.sql(sql_stmt)

    # ------------------------------------------------------------------ #
    # Public entry-point
    # ------------------------------------------------------------------ #
    def execute(self) -> None:
        """Execute the CreateTable task to create a Delta table if it doesn't exist."""
        spark = self.context.spark

        # 1. Table already exists?
        if spark.catalog.tableExists(self._full_table_name):
            self.context.logger.info(f"Table '{self._full_table_name}' already exists - nothing to do.")
            return

        # 2. Derive column list
        cols_ddl = self.table_model.schema_ddl
        # 3. Assemble CREATE statement
        create_stmt = (
            f"CREATE TABLE {self._full_table_name} ("
            f"{cols_ddl}{self._audit_columns_suffix()}"
            f") {self._liquid_cluster_clause()} {self._partition_clause()} {self._comment_clause()} "
            "TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')"
        )

        # 4. Execute with concurrency guard
        try:
            self.context.logger.info(create_stmt)
            spark.sql(create_stmt)
        except AnalysisException as exc:
            if exc.getErrorClass() == "TABLE_OR_VIEW_ALREADY_EXISTS" and spark.catalog.tableExists(
                self._full_table_name
            ):
                self.context.logger.info(f"Table '{self._full_table_name}' was created concurrently - fine.")
            else:
                raise

        # 5. Add pks if any
        if self.table_model.primary_keys:
            self._add_primary_keys()

        # 6. add CHECK constraints declared on the model
        if self.table_model.constraints:
            self._add_constraints()
        # 7. Update table metadata
        self.context.logger.info("Updating table metadata.")
        self.table_model.update_metadata(self.context)
