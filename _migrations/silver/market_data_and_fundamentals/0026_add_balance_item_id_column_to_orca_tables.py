# Databricks notebook source
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

spark = SparkSession.builder.getOrCreate()

# COMMAND ----------


catalog_prefix = dbutils.widgets.get("env_catalog_identifier")  # type: ignore # noqa: F821
schema_prefix = dbutils.widgets.get("schema_prefix")  # type: ignore # noqa: F821

tables_and_schemas = [
    {"table_name": "fcr_pos_awarded_capacity_power_orca", "schema_name": "market_data_and_fundamentals"},
    {"table_name": "fcr_neg_awarded_capacity_power_orca", "schema_name": "market_data_and_fundamentals"},
    {"table_name": "afrr_neg_awarded_capacity_power_orca", "schema_name": "market_data_and_fundamentals"},
    {"table_name": "afrr_pos_awarded_capacity_power_orca", "schema_name": "market_data_and_fundamentals"},
]
primary_keys = "signal_id, balance_item_id, timestamp"

for table_info in tables_and_schemas:
    table_name = table_info["table_name"]
    schema_name = table_info["schema_name"]

    if spark.catalog.tableExists(f"{catalog_prefix}silver.{schema_prefix}{schema_name}.{table_name}"):
        df = spark.read.table(f"{catalog_prefix}silver.{schema_prefix}{schema_name}.{table_name}")

        if "balance_item_id" not in df.columns:
            df = df.withColumn("balance_item_id", lit("2"))
            df = df.withColumn("value", col("value").cast("double"))

        # Select columns in specified order
        df = df.select(
            "signal_id",
            "balance_item_id",
            "timestamp",
            "value",
            "unit_type",
            "license",
            "created_at",
            "created_by",
            "updated_at",
            "updated_by",
        )

        # Drop existing table
        spark.sql(f"DROP TABLE IF EXISTS {catalog_prefix}silver.{schema_prefix}{schema_name}.{table_name}")

        # Create new table with table_id and primary key constraint
        spark.sql(f"""
            CREATE TABLE {catalog_prefix}silver.{schema_prefix}{schema_name}.{table_name} (
                table_id BIGINT NOT NULL GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),
                signal_id STRING,
                balance_item_id STRING,
                timestamp TIMESTAMP,
                value DOUBLE,
                unit_type STRING,
                license STRING,
                created_at TIMESTAMP,
                created_by STRING,
                updated_at TIMESTAMP,
                updated_by STRING,
                CONSTRAINT pk_{table_name} PRIMARY KEY({primary_keys})
            )
        """)

        # Insert data
        df.write.format("delta").mode("append").saveAsTable(
            f"{catalog_prefix}silver.{schema_prefix}{schema_name}.{table_name}"
        )
