# Databricks notebook source
import json

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# COMMAND ----------


catalog_prefix = dbutils.widgets.get("env_catalog_identifier")  # type: ignore # noqa: F821
schema_prefix = dbutils.widgets.get("schema_prefix")  # type: ignore # noqa: F821
schema = "deal_data"
tables = ["volume_hourly_endur"]


def get_table_ddl_script(catalog: str, schema: str, target_table: str) -> str:
    """Generate the DDL script for creating a table with an auto-increment identity column.

    Args:
        catalog (str): The catalog name.
        schema (str): The schema name.
        target_table (str): The target table name.

    Returns:
        str: The DDL script for creating the table.
    """
    src_table = f"{catalog}.{schema}.{target_table}"
    df = spark.sql(
        f"select column_name from `system_tables_sharing`.system_tables_information_schema.constraint_column_usage "
        f"where lower(table_name) = '{target_table}' and lower(table_schema) = '{schema}' "
        f"and lower(table_catalog) = '{catalog}' and lower(constraint_name) = 'pk_{target_table}'"
    ).collect()
    pk = [row.column_name for row in df]
    result = []
    primary_key_str = ""
    if any(pk):
        primary_key_str = f", CONSTRAINT pk_{target_table} PRIMARY KEY({', '.join(pk)})"
    table_id_prefix_clause = "table_id BIGINT not null GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),"
    # Retrieve schema from schema(if provided in dlt config), otherwise from source table
    schema = spark.read.table(src_table).schema.json()
    schema_json = json.loads(schema)

    # Build the DDL string from schema fields
    for metadata in schema_json["fields"]:
        if isinstance(metadata["type"], dict):
            field_type = f"{metadata['type']['type']}<{metadata['type']['elementType']}>"
        else:
            field_type = metadata["type"]
        not_nullble = " NOT NULL" if metadata["name"] in pk else ""
        comment = f" COMMENT '{metadata['metadata']['comment']}'" if "comment" in metadata["metadata"] else ""
        result.append(f"{metadata['name']} {field_type}{not_nullble}{comment}")

    return table_id_prefix_clause + ", ".join(result) + primary_key_str


for table in tables:
    full_table = f"{catalog_prefix}silver.{schema_prefix}{schema}.{table}"
    print(f"Processing table: {full_table}")
    if spark.catalog.tableExists(full_table):
        ddl = get_table_ddl_script(f"{catalog_prefix}silver", f"{schema_prefix}{schema}", table)
        query = f"""CREATE TABLE IF NOT EXISTS {full_table} (
                    {ddl}
                )
                TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')"""
        spark.sql(f"ALTER TABLE {full_table} DROP CONSTRAINT IF EXISTS pk_{table}")
        spark.sql(f"ALTER TABLE {full_table} RENAME TO {full_table}_backup")
        spark.sql(query)
        (spark.read.table(f"{full_table}_backup").write.format("delta").mode("append").saveAsTable(f"{full_table}"))
        spark.sql(f"DROP TABLE {full_table}_backup")
