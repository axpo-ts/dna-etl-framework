# Databricks notebook source
import json

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()


# COMMAND ----------


catalog_prefix = dbutils.widgets.get("env_catalog_identifier")  # type: ignore # noqa: F821
schema_prefix = dbutils.widgets.get("schema_prefix")  # type: ignore # noqa: F821
schema = "volue"

tables = ["hydro_inflow", "precipitation", "residual_load"]


primary_keys = {
    "hydro_inflow": ["curve_name", "delivery_start"],
    "precipitation": ["curve_name", "delivery_start"],
    "residual_load": ["curve_name", "delivery_start"],
}


def get_table_ddl_script(catalog: str, schema: str, target_table: str) -> str:
    """Generate the DDL script for creating a table with an auto-increment identity column.

    Args:
        catalog (str): The catalog name.
        schema (str): The schema name.
        target_table (str): The target table name.

    Returns:
        str: The DDL script for creating the table.
    """
    src_table = f"{catalog}.{schema}.{target_table}"
    df = spark.sql(
        f"select column_name, constraint_name from"
        f"`system_tables_sharing`.system_tables_information_schema.constraint_column_usage "
        f"where lower(table_name) = '{target_table}' and lower(table_schema) = '{schema}' "
        f"and lower(table_catalog) = '{catalog}' "
        f"and ("
        f"lower(constraint_name) = 'pk_{target_table}' or "
        f"lower(constraint_name) = '{target_table}_pk')"
    ).collect()
    pk = [row.column_name for row in df]
    constraint_names = list({row.constraint_name for row in df})
    if not pk:
        pk = primary_keys.get(target_table, [])
    primary_key_str = ""
    if pk:
        constraint_name = constraint_names[0] if constraint_names else f"pk_{target_table}"
        primary_key_str = f", CONSTRAINT {constraint_name} PRIMARY KEY({', '.join(pk)})"
    table_id_prefix_clause = "table_id BIGINT not null GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1)"
    # Retrieve schema from schema(if provided in dlt config), otherwise from source table
    schema = spark.read.table(src_table).schema.json()
    schema_json = json.loads(schema)

    # Build the DDL string from schema fields
    main_fields = []
    extra_fields = []
    for metadata in schema_json["fields"]:
        col_name = metadata["name"]
        if col_name in ["created_at", "created_by", "updated_at", "updated_by", "table_id"]:
            if isinstance(metadata["type"], dict):
                field_type = f"{metadata['type']['type']}<{metadata['type']['elementType']}>"
            else:
                field_type = metadata["type"]
            not_nullable = " NOT NULL" if col_name in pk else ""
            comment = f" COMMENT '{metadata['metadata']['comment']}'" if "comment" in metadata["metadata"] else ""
            extra_fields.append(f"{col_name} {field_type}{not_nullable}{comment}")
        elif col_name != "reference_date":
            if isinstance(metadata["type"], dict):
                field_type = f"{metadata['type']['type']}<{metadata['type']['elementType']}>"
            else:
                field_type = metadata["type"]
            not_nullable = " NOT NULL" if col_name in pk else ""
            comment = f" COMMENT '{metadata['metadata']['comment']}'" if "comment" in metadata["metadata"] else ""
            main_fields.append(f"{col_name} {field_type}{not_nullable}{comment}")

    # Ensure table_id is always at the end, even if not in schema
    table_id_comment = " COMMENT 'DnA Platform internal unique identifier for the record'"
    if not any(f.startswith("table_id") for f in extra_fields):
        extra_fields.append(table_id_prefix_clause + table_id_comment)
    else:
        extra_fields = [f for f in extra_fields if not f.startswith("table_id")] + [
            table_id_prefix_clause + table_id_comment
        ]

    return ", ".join(main_fields + extra_fields) + primary_key_str


def get_table_comment(table_name: str) -> str:
    """Retrieves the comment associated with a specified table in Spark.

    Args:
        table_name (str): The name of the table for which to retrieve the comment.

    Returns:
        str: The comment of the table in the format "COMMENT 'comment_text'",
                or an empty string if no comment is found.
    """
    tbl_metadata = spark.sql(f"DESCRIBE EXTENDED {table_name}").filter('col_name = "Comment"').first()
    table_comment = f"COMMENT '{tbl_metadata.data_type}'" if tbl_metadata and tbl_metadata.data_type else ""
    return table_comment


for table in tables:
    full_table = f"{catalog_prefix}silver.{schema_prefix}{schema}.{table}"
    print(f"processing {full_table}")
    if spark.catalog.tableExists(full_table):
        print(f"processing table {table}")
        columns = spark.catalog.listColumns(full_table)
        partition_columns = [col.name for col in columns if col.isPartition]
        partition_clause = f"PARTITIONED BY ({', '.join(partition_columns)})" if partition_columns else ""
        if any(col.name == "reference_date" for col in columns):
            ddl = get_table_ddl_script(f"{catalog_prefix}silver", f"{schema_prefix}{schema}", table)
            tbl_comment = get_table_comment(full_table)
            query = f"""CREATE TABLE IF NOT EXISTS {full_table} (
                    {ddl}
                ) {partition_clause} {tbl_comment}
                TBLPROPERTIES(
                    'delta.feature.allowColumnDefaults' = 'supported'
                )"""
            print(query)
            spark.sql(f"ALTER TABLE {full_table} DROP CONSTRAINT IF EXISTS pk_{table}")
            spark.sql(f"ALTER TABLE {full_table} RENAME TO {full_table}_backup")
            spark.sql(query)
            (
                spark.read.table(f"{full_table}_backup")
                .drop("table_id", "reference_date")
                .write.format("delta")
                .mode("append")
                .saveAsTable(full_table)
            )
            tags_df = spark.sql(f"""SELECT tag_name, tag_value FROM system.information_schema.table_tags
                                WHERE catalog_name = '{catalog_prefix}silver'
                                AND schema_name = '{schema_prefix}{schema}'
                                AND table_name = '{table}_backup'""")
            for row in tags_df.collect():
                tag_name = row["tag_name"]
                tag_value = row["tag_value"]
                spark.sql(
                    f"""
                    ALTER TABLE {catalog_prefix}silver.{schema_prefix}{schema}.{table}
                    SET TAGS ("{tag_name}" = '{tag_value}')
                    """
                )
            spark.sql(f"DROP TABLE {full_table}_backup")
