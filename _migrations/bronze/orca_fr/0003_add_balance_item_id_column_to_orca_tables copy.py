# Databricks notebook source
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

spark = SparkSession.builder.getOrCreate()

# COMMAND ----------


catalog_prefix = dbutils.widgets.get("env_catalog_identifier")  # type: ignore # noqa: F821
schema_prefix = dbutils.widgets.get("schema_prefix")  # type: ignore # noqa: F821

tables_and_schemas = [
    {"table_name": "fcr_pos_awarded_capacity_power_orca", "schema_name": "orca_fr"},
    {"table_name": "fcr_neg_awarded_capacity_power_orca", "schema_name": "orca_fr"},
    {"table_name": "afrr_neg_awarded_capacity_power_orca", "schema_name": "orca_fr"},
    {"table_name": "afrr_pos_awarded_capacity_power_orca", "schema_name": "orca_fr"},
    {"table_name": "fcr_reaction_power_orca", "schema_name": "orca_fr"},
    {"table_name": "afrr_reaction_power_orca", "schema_name": "orca_fr"},
    {"table_name": "afrr_request_power_orca", "schema_name": "orca_fr"},
    {"table_name": "schedule_power_orca", "schema_name": "orca_fr"},
    {"table_name": "effective_power_orca", "schema_name": "orca_fr"},
    {"table_name": "realised_schedule_power_orca", "schema_name": "orca_fr"},
]
primary_keys = "signal_id, balance_item_id, timestamp"

for table_info in tables_and_schemas:
    table_name_full = f"{catalog_prefix}bronze.{schema_prefix}{table_info['schema_name']}.{table_info['table_name']}"
    print(f"Processing table {table_name_full}")
    schema_name = table_info["schema_name"]
    print(f"Processing table {table_name_full}")
    if spark.catalog.tableExists(table_name_full):
        print(f"Table {table_name_full} already exists")
        df = spark.read.table(table_name_full)

        if "balance_item_id" not in df.columns:
            df = df.withColumn("balance_item_id", lit("2"))
            df = df.withColumn("value", col("value").cast("double"))
            df = df.withColumn("signal_id", col("signalId"))

            # Select columns in specified order
            df = df.select(
                "signal_id",
                "balance_item_id",
                "timestamp",
                "value",
                "unit_type",
                "license",
                "created_at",
                "created_by",
                "updated_at",
                "updated_by",
            )

            # Drop existing table
            spark.sql(f"DROP TABLE IF EXISTS {table_name_full}")

            # Create new table with table_id and primary key constraint
            spark.sql(f"""
                CREATE TABLE {table_name_full} (
                    table_id BIGINT NOT NULL GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),
                    signal_id STRING,
                    balance_item_id STRING,
                    timestamp TIMESTAMP,
                    value DOUBLE,
                    unit_type STRING,
                    license STRING,
                    created_at TIMESTAMP,
                    created_by STRING,
                    updated_at TIMESTAMP,
                    updated_by STRING,
                    CONSTRAINT pk_{table_info["table_name"]} PRIMARY KEY({primary_keys})
                )
            """)

            # Insert data
            df.write.format("delta").mode("append").saveAsTable(f"{table_name_full}")
